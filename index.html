<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-PJTZBF56');</script>
    <!-- End Google Tag Manager -->  

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>WAVE</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/images/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>   
    
<body data-gr-c-s-loaded="true">
    
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PJTZBF56"
        height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models
            <br /><br />
            <small>
                Coming soon...
            </small>
            <br /><br />
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ppwangyc.github.io">
                          Yanchen Wang
                        </a><sup>*1</sup>
                    </li>
                    <li>
                        <a href="https://www.cogtlab.com/copy-of-current-team-1">
                          Adam Turnbull
                        </a><sup>*1</sup>
                    </li>                    
                   <li>
                        <a href="https://tiangexiang.github.io/">
                          Tiange Xiang
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://sites.google.com/u.rochester.edu/dracoxu/home">
                          Yunlong Xu
                        </a><sup>3</sup>
                    </li> 
                    <li>
                        <a href="https://www.cogtlab.com/copy-of-current-team-1">
                          Sa Zhou
                        </a><sup>1</sup>
                    </li> 
                    <li>
                        <a href="https://scholar.google.com/citations?user=WgunFCMAAAAJ&hl=en">
                            Adnan Masoud
                        </a><sup>4</sup>
                    </li> 
                    <li>
                        <a href="https://www.shekoofehazizi.com/">
                            Shekoofeh Azizi
                        </a><sup>5</sup>
                    </li> 
                    <li>
                        <a href="https://profiles.stanford.edu/f-lin">
                            Feng Vankee Lin
                        </a><sup>†1</sup>
                    </li>   
                    <li>
                        <a href="https://stanford.edu/~eadeli/">
                            Ehsan Adeli
                        </a><sup>†1,2</sup>
                    </li>                       
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>Standford University, Department of Psychiatry and Behavioral Sciences
                    </li>
                    <br>
                    <li>
                        <sup>2</sup>Standford University, Department of Computer Science
                    </li>
                    <br>
                    <li>
                        <sup>3</sup>The University of Chicago, Department of Neurobiology
                    </li>
                    <br>
                    <li>
                        <sup>4</sup>UST
                    </li>
                    <br>
                    <li>
                        <sup>5</sup>Google DeepMind
                    </li>
                    <br>
                    <li>
                        <sup>*</sup>Equal Contribution <sup>†</sup>Equal Senior Authorship
                    </li>
                    <br>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2411.07121" target='_blank'>
                        <img src="index_files/images/paper.jpg" height="80px"><br>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/ppwangyc/wave">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://huggingface.co/collections/PPWangyc/wave-66808dc22b1f40e838c43b97">
                        <img src="./index_files/images/hugging_face.png" height="80px"><br>
                            <h4><strong>HuggingFace</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/figure_1.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural decoding, the process of understanding how brain activity corresponds to different stimuli, has been a primary objective in cognitive sciences.
  Over the past three decades, advancements in functional Magnetic Resonance Imaging (fMRI) and machine learning have greatly improved our ability to map visual stimuli to brain activity, especially in the visual cortex. Concurrently, research has expanded into decoding more complex processes like language and memory across the whole brain, utilizing techniques to handle greater variability and improve signal accuracy. We argue that "seeing" involves more than just mapping visual stimuli onto the visual cortex; it engages the entire brain, as various emotions and cognitive states can emerge from observing different scenes. In this paper, we develop AI algorithms to enhance our understanding of visual processes by incorporating whole-brain activation maps while individuals are exposed to visual stimuli. We utilize large-scale fMRI encoders and Image generative models (encoders & decoders) pre-trained on large public datasets, which are then fine-tuned through Image-fMRI contrastive learning. Our models hence can decode visual experience across the entire cerebral cortex, surpassing the traditional confines of the visual cortex. Using a public dataset (BOLD5000), we first compare our method with state-of-the-art approaches to decoding visual processing and show improved predictive semantic accuracy by <b>43%</b>. Further analyses suggest that higher cortical regions contribute more to decoding complex visual scenes. Additionally, we implemented zero-shot imagination decoding on an extra validation dataset, achieving a p-value of <b>0.0206</b>, which substantiates the model's capability to capture semantic meanings across various scenarios. These findings underscore the potential of employing comprehensive models in enhancing the nuanced interpretation of semantic processes.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>Whole-brain</b> scale visual decoding using fMRI data.
                        </li>
                        <li>
                            SOTA 50-way top-1 classification accuracy (semantic) on BOLD5000 dataset, surpassing the previous best by <b>43%</b>.
                        </li>
                        <li>
                            Succesfully Decode the images <b>without visual network</b> (Yeo 7 networks).
                        </li>
                        <li>
                            Whole brain clustering analysis shows that higher cortical regions contribute more to decoding complex visual scenes.
                        </li>
                        <li>
                            Validation on an extra dataset with <b>zero-shot imagination decoding</b>, achieving a p-value of <b>0.0206</b>.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    WAVE
                </h3>
                <img src="./index_files/images/figure_2.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    The depicted architecture illustrates a two-part training approach for the model. Part A focuses on contrastive learning, where knowledge is distilled using three modalities: fMRI, text, and images. Part B advances to diffusion model training, fine-tuning a specialized prior to transform fMRI latents into image latents. Icons of locks indicate modules that were frozen during the training phase.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <img src="./index_files/images/figure_3.png" class="img-responsive" alt="result with sota"><br>
                <p class="text-justify">
                    <b>Comparison of fMRI data decoding using WAVE and other advanced methods.</b> a, Reconstructed images using individual model settings for WAVE (Whole Brain), MindEye, and Mind-Vis across the fMRI visual cortex. b, Universal settings across all four subjects in the BOLD5000 dataset. This panel demonstrates the generalization capability of WAVE, MindEye, and Mind-Vis among different subjects. c, The saliency map of whole visual network masked WAVE model, which showed the top 25% regions of interests. d, Reconstructed images from <b>masked visual network</b> fMRI data. We set the visual region values to 0 as a masked. The visual cortex masked reconstructed image semantic accuracy is <b>18.98%</b> averaged across 4 subjects.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Whole Brain Clustering Analysis
                </h3>
                <img src="./index_files/images/figure_4.png" class="img-responsive" alt="clustering analysis"><br>
                <p class="text-justify">
                    <b>Visualization of Post-Hoc Whole-Brain Visual Clustering Analysis.</b> 
                        a, t-SNE visualization of image embeddings, differentiated by five clusters identified via K-Means, each represented in unique colors. b-f, Each subfigure presents word clouds generated from image labels, with word sizes proportional to their frequency of occurrence within the cluster. The subfigure titles, generated by ChatGPT-4, summarize the thematic essence of each cluster. Accompanying each word cloud are selected image samples and a whole-brain saliency map highlighting the top 20 regions of interest relevant to the cluster. h, Quantitative analysis showing the distribution of regions across the Yeo 7 networks for each of the five clusters, providing insights into the network-based localization of visual processing associated with different categories.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Zero-Shot Imagination Decoding
                </h3>
                <img src="./index_files/images/figure_5.png" class="img-responsive" alt="zero-shot"><br>
                <p class="text-justify">
                    <b>WAVE Zero-Shot brain imagination decoding results</b> a, Scenario-based scale analysis utilizing two-way-identification accuracy. This analysis measures the cosine distance of each scenario's features to those in the training dataset~(BOLD5000), showing how scenario proximity affects decoding accuracy. b, Example of WAVE model zero-shot reconstructed images from imagination recording fMRI sessions. The text stimuli describing the museum scenario is presented above the example image, illustrating the model's capability to generate visual reconstructions based on described scenarios.
            </div>
        </div>

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@article{wang2024decoding,
  title={Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models},
  author={Wang, Yanchen and Turnbull, Adam and Xiang, Tiange and Xu, Yunlong and Zhou, Sa and Masoud, Adnan and Azizi, Shekoofeh and Lin, Feng Vankee and Adeli, Ehsan},
  journal={arXiv preprint arXiv:2411.07121},
  year={2024}
}</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                This research is supported by the HAI-Google Research Award, UST, HAI Hoffman-Yee Grant, and National Institutes of Health grants NIH U24AG072701 and R01AG089169. We also greatly thank Yurong Liu from New York University, for providing figure suggestions and proofreading. The website template was borrowed from <a href="https://curvenet.github.io">here</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>